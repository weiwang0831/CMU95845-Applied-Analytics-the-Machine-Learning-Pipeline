---
title: "HW2"
author: "Wei_Wang_ww5"
date: "March 15, 2019"
output: html_document
---
Name: Wei Wang 
andrew ID: ww5

## Part 1: Randomized trial for strokes (12 points)

### Preliminaries
1a.

- **Y:** What was the definition of the primary outcome in this study?
- What is (are) the variable name(s) for the outcome?

Answer: The definition of the primary outcome is death within 14 days and death or dependency at 6 months
The variable names of the outcome is: **DDEAD(Dead on discharge form)** and **FDEAD(Dead at six month follow-up (Y/N))**

- **U:** what is (are) the variable name(s) for the intervention, and what is (are) their possible values?

The intervention in this study include the what descibe below: Half the patients were allocated unfractionated heparin (5000 or 12500 IU bd [twice daily]) and half were allocated avoid heparin; and, in a factorial design, half were allocated aspirin 300 mg daily and half avoid aspirin?.

Therefore the variables include: 
- RHEP24 Heparin within 24 hours prior to randomisation (Y/N)
- RASP3 Aspirin within 3 days prior to randomisation (Y/N)
- DASP14 Aspirin given for 14 days or till death or discharge (Y/N)
- DASPLT Discharged on long term aspirin (Y/N)
- DLH14 Low dose heparin given for 14 days or till death/discharge (Y/N)
- DMH14 Medium dose heparin given for 14 days or till death/discharge (Y/N)
- DHH14 Medium dose heparin given for 14 days etc in pilot (combine with above)

- **V, W:** describe the covariates included and the population being studied. Be specific where possible.

Covariates can be the the patients' personal information including Age, sex, and the Final diagnosis of initial event
DDIAGISC Ischaemic stroke; DDIAGHA Haemorrhagic stroke; DDIAGUN Indeterminate stroke; DNOSTRK Not a stroke; DNOSTRKX Comment on above.

The population of the study is: 19435 patients with suspected acute ischaemic stroke entering 467 hospitals in 36 countries were randomised within 48 hours of symptom onset.


1b. Provide descriptive statistics for in groups of {aspirin, no aspirin} use, including information on age, gender, systolic blood pressure, and conscious state. In clinical literature, this information is often referred to as "Table 1".

```{r}
#set environment
loadlibs = function(libs) {
  for(lib in libs) {
    class(lib)
    if(!do.call(require,as.list(lib))) {install.packages(lib)}
    do.call(require,as.list(lib))
  }
}
libs = c("dplyr","mice","randomForest","adabag","gbm","pROC","gsubfn","chron")
loadlibs(libs)

#read dataset
stroke=read.csv("IST_corrected.csv",header = TRUE)
stroke=stroke[stroke$DASP14!="n",]
stroke=stroke[stroke$DASP14!="y",]
```

#### statistics description group by

```{r}
aggregate( . ~ RASP3, data=stroke[c("AGE","RCONSC","RSBP","RASP3")], FUN=range)
aggregate( . ~ RASP3, data=stroke[c("AGE","RSBP","RASP3")], FUN=mean)
stroke %>% 
  group_by(RASP3,SEX) %>%
  summarise(Freq = n()) %>%
  filter(RASP3!="")
stroke %>% 
  group_by(RASP3,RCONSC) %>%
  summarise(Freq = n()) %>%
  filter(RASP3!="")
```

### Machine learning analysis
```{r}
# split the dataset into 50-50
smp_size <- floor(0.5 * nrow(stroke))
## set the seed to make your partition reproducible
set.seed(123)
train_stroke <- sample(seq_len(nrow(stroke)), size = smp_size)
train <- stroke[train_stroke, ]
test <- stroke[-train_stroke, ]
```

####1c. Let our outcome of interest be "dead or dependent at 6 months", i.e. so that we have a binary classification problem. What percent of patients are dead or dependent at 6 months in your train set and test set?

The variable is chosen to be FDEAD
```{r}
length(train[train$FDEAD=="Y",])/length(train$FDEAD)
length(test[test$FDEAD=="Y",])/length(test$FDEAD)
```

####1d. Choose which variables to include in your model.

The below variables should be removed:

DDEAD, DDEADD, DDEADC, DDEADX, DALIVE, DALIVED, DPLACE, these are all the 14 days variables.

FDEADX, comment on death is unrelated
```{r}
#delete columns useless
delete=c("FDEADX","DDEAD", "DDEADD", "DDEADC", "DDEADX", "DALIVE", "DALIVED", "DPLACE","COUNTRY",
         "CNTRYNUM","NCCODE","RDATE","DSIDEX","DNOSTRKX","DMAJNCHX",
         "HOURLOCAL","MINLOCAL","DAYLOCAL","FSOURCE","ID","EXPDD","EXPD14","EXPD6","SET14D",
         "ID14","OCCODE","SETASPLT","ID","DIED","DEAD1","DEAD2","DEAD3","DEAD4","DEAD5","DEAD6","DEAD7","DEAD8",
         "FDEADC","FDENNIS","FPLACE",
         "RDEF1","RDEF2","RDEF3","RDEF4","RDEF5","RDEF6","RDEF7","RDEF8")

train=train[, !(colnames(train) %in% delete), drop=FALSE] 
test=test[, !(colnames(test) %in% delete), drop=FALSE]
```

####1e. Of the remaining variables, decide whether to exclude variables with missing data, impute them, and/or use indicator variables.

#####data clean and imputate
```{r}
#check number of Nans in the dataset
null=sapply(train, function(x) sum(is.na(x)))
null=null[null>0]
null
```

As we can see from the result, the above columns have high number of NA values, For the date that is missing mostly, I remove the columns. For column **FDEADC**, it relates to the cause of death, for NA record, I replace them with 0, which refers to "unknown". Column **FU1_RECD, FU2_DONE, FU1_COMP, TD** can be impute since it shows some correlation with the outcome

```{r}
delete2=c("DMAJNCHD", "DSIDED","DRSISCD","DRSHD","DRSUNKD","DPE","DPED","FLASTD","FDEADD","DRSUNK",
          "FAP","FAP","FRECOVER","FOAC","CMPLHEP","DRSH","DNOSTRKX","TD")
train=train[, !(colnames(train) %in% delete2), drop=FALSE] 
test=test[, !(colnames(test) %in% delete2), drop=FALSE] 
#train$FDEADC <- replace(train$FDEADC,is.na(train$FDEADC),0)
#test$FDEADC <- replace(test$FDEADC,is.na(test$FDEADC),0)
train$ONDRUG <- replace(train$ONDRUG,is.na(train$ONDRUG),0)
test$ONDRUG <- replace(test$ONDRUG,is.na(test$ONDRUG),0)

#imputate train data
mtrain = mice(train %>% 
              select(c("FU1_RECD","FU2_DONE","FU1_COMP")) %>% 
              mutate_if(is.character, as.factor),m=1,maxit = 1) 
train_tmp = mice::complete(mtrain) %>% as_tibble()
# Rename columns to indicate imputation
names(train_tmp) = lapply(names(train_tmp), paste0, "_imputed")

#imputate test data
mtest = mice(test %>% 
                select(c("FU1_RECD","FU2_DONE","FU1_COMP")) %>% 
                mutate_if(is.character, as.factor),m=1,maxit = 1) 
test_tmp = mice::complete(mtest) %>% as_tibble()
# Rename columns to indicate imputation
names(test_tmp) = lapply(names(test_tmp), paste0, "_imputed")

#reform the two dataset
delete3=c("FU1_RECD","FU2_DONE","FU1_COMP","TD","ONDRUG")
train=train[, !(colnames(train) %in% delete3), drop=FALSE] 
test=test[, !(colnames(test) %in% delete3), drop=FALSE]
train=cbind(train,train_tmp)
test=cbind(test,test_tmp)

#clean dependent variable
train=train[train$FDEAD!="U",]
train=train[train$FDEAD!="",]
train$FDEAD=droplevels(train$FDEAD)

test=test[test$FDEAD!="U",]
test=test[test$FDEAD!="",]
test$FDEAD=droplevels(test$FDEAD)
```

#####logistic regression

```{r}
mylogit <- glm(FDEAD ~., data = train,family = "binomial")
```

#####random forest
```{r}
rf <- randomForest(FDEAD ~ ., data = train, ntree = 500, mtry = 6, importance = TRUE)
summary(rf)
```

#####boosting
```{r}
boost <- boosting(FDEAD~., data=train, boos=TRUE, mfinal=3)
summary(boost)
```

#####gradient boosting
```{r}
gbm <- gbm(FDEAD~., data=train,distribution = "gaussian")
#n.trees = 10000,shrinkage = 0.01, interaction.depth = 4
summary(gbm) #Summary gives a table of Variable Importance and a plot of Variable Importance
```

####1f. Construct an ROC
```{r}
#ROC for multiple model
rf_pred=data.frame(predict(rf, test, type="prob"))
lg_pre=predict(mylogit,newdata=test,type=c("response"))
boost_pred=predict(boost,test,type=c("response"))
boost_pred=data.frame(boost_pred$prob)
boost_pred=boost_pred$X2
gbm_pred=predict(gbm,test,n.trees = 100,type = c("response"))

roc_rose <- plot(roc(test$FDEAD,lg_pre), print.auc = TRUE, col = "blue")
roc_rose <- plot(roc(test$FDEAD, rf_pred$Y), print.auc = TRUE, 
                 col = "green", print.auc.y = .4, add = TRUE)
roc_rose <- plot(roc(test$FDEAD,boost_pred), print.auc = TRUE, 
                 col = "red", print.auc.y = .3, add = TRUE)
roc_rose <- plot(roc(test$FDEAD,gbm_pred), print.auc = TRUE, 
                 col = "yellow", print.auc.y = .2, add = TRUE)
```

####1g. Report the variable importance
```{r}
# variable importance
importance(rf)
library(caret)
varImpPlot(rf,type=2)
```

## Part 2. Basis functions and regularization for daily glucoses [8 points]

read dataset
```{r}
#read data
item=read.csv("d_labitems.csv",header = TRUE)
events=read.csv("labevents.csv",header = TRUE)
data=events[events$subject_id=="13033",]
data=data[data$itemid=="50112",]

summary(data$valuenum)
boxplot(data$valuenum)
```

####2a. Extract the glucose data
```{r}

library(ggplot2)
library(scales)
library(lubridate)
data$Date <- as.Date(data$charttime)
data$time <- times(strftime(data$charttime,"%H:%M:%S"))
ggplot(data, aes(charttime, valuenum)) + 
  geom_point() + 
  #scale_x_datetime(breaks=date_breaks("2 month"), labels="%b-%d-%Y") + 
  #theme(axis.text.x=element_text(angle=90))+
  geom_hline(yintercept = mean(data$valuenum), color="blue")+
  geom_hline(yintercept = 72, color="red")+
  geom_hline(yintercept = 140, color="red")
```

####2b. Split the data into training set and test set
I use period = 0.5, which means 12 hours (24*0.5)
```{r}
#split data
bld_test=data[data$Date>"3417-5-1",]
bld_train=data[data$Date<="3417-5-1",]

#sine consine basic function
period = 0.5
K = 5

# Function for basis expansion of {sin(kx),cos(kx)} for i = 1 to K
sincos = function(dat, variable="time", period=2*pi, K=10) {
  data = dat
  for(i in 1:K) {
    data[[paste0("sin_",i)]] = sin(data[[variable]]*i*2*pi/period)
  }
  for(i in 1:K) {
    data[[paste0("cos_",i)]] = cos(data[[variable]]*i*2*pi/period)
  }
  # data$intercept = 1  # not necessary if using with models that use intercepts
  data
}
```

####2c. Use cv.glmnet to learn a daily trend for the individual on the training set. 
Plot the coefficient profile with lambda on the x-axis. Report the lambda that performed best in CV (use lambda.min for this exercise).
```{r}
bld_train1 = sincos(bld_train, period=period, K=K)
bld_train1 = bld_train1 %>% dplyr::select(-charttime)%>% dplyr::select(-time)%>% dplyr::select(-Date)

# Learn a linear model, regularized
library(glmnet)  # Does regularization with (generalized) linear models
lasso = cv.glmnet(x = bld_train1 %>% select(-c(subject_id,hadm_id,icustay_id,itemid,value,valuenum,valueuom,flag)) 
                  %>% as.matrix(),
                  y=bld_train1$valuenum, family="gaussian")
lambda = lasso$lambda.min
lambda
plot(lasso$glmnet.fit, "lambda")
```


####2d. plot
```{r}
#predict
bld_test1 = sincos(bld_test, period=period, K=K)
bld_test2 = bld_test1 %>% dplyr::select(-charttime)%>% dplyr::select(-time)%>% dplyr::select(-Date)

self_data=data.frame(seq(ISOdate(2000,1,31), by = "min", length.out = 1440))
self_data$time=times(strftime(self_data$seq.ISOdate.2000..1..31...by....min...length.out...1440.,"%H:%M:%S"))
self_data1=sincos(self_data, period=period, K=K)
self_data2=self_data1%>% dplyr::select(-time)%>% dplyr::select(-seq.ISOdate.2000..1..31...by....min...length.out...1440.)

self_data1[["ylasso"]] = predict(lasso,
                                 self_data2
                                #%>% select(-c(subject_id,hadm_id,icustay_id,itemid,value,valuenum,valueuom,flag)) 
                                %>% as.matrix(),
                              s = c(lasso$lambda.min))
# Plot
ggplot(data=bld_test1, aes(time,valuenum)) + geom_point()+
  geom_line(data = self_data1, aes(x=time,y=ylasso,color="red"))

```


####2e. What percent of the variation is explained by your model compared to using the training set mean?
Based on the claculation below, around 94%
```{r}
train_mean=mean(bld_train$valuenum)
#sum of square error
ssq=sum((bld_test$valuenum-train_mean)^2)
rsq=sum((bld_test1$valuenum-bld_test1$ylasso)^2)
r2=rsq/ssq
r2

```

####2f. Make a statement about the daily variation of glucose in this individual. In particular, according to your model, when is it lowest? When is it highest?

It is high in the mid night, and in the morning about 6am, it is low. Until 12:00pm, the value is slowly increasing, and drop  to lowest at 6pm.


